{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(3, 30)\n",
      "(3, 94)\n",
      "(9, 8)\n",
      "(9, 100)\n",
      "(9, 155)\n",
      "(9, 187)\n",
      "(9, 188)\n",
      "(20, 49)\n",
      "(20, 51)\n",
      "(20, 90)\n",
      "[[30, 94], [8, 100, 155, 187, 188], [49, 51, 90]]\n",
      "x\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "maxpool\n",
      "layer1_0_conv1\n",
      "layer1_0_bn1\n",
      "layer1_0_relu\n",
      "layer1_0_conv2\n",
      "layer1_0_bn2\n",
      "layer1_0_relu_1\n",
      "layer1_0_conv3\n",
      "layer1_0_bn3\n",
      "layer1_0_downsample_0\n",
      "layer1_0_downsample_1\n",
      "add\n",
      "layer1_0_relu_2\n",
      "layer1_1_conv1\n",
      "layer1_1_bn1\n",
      "layer1_1_relu\n",
      "layer1_1_conv2\n",
      "layer1_1_bn2\n",
      "layer1_1_relu_1\n",
      "layer1_1_conv3\n",
      "layer1_1_bn3\n",
      "add_1\n",
      "layer1_1_relu_2\n",
      "layer1_2_conv1\n",
      "layer1_2_bn1\n",
      "layer1_2_relu\n",
      "layer1_2_conv2\n",
      "layer1_2_bn2\n",
      "layer1_2_relu_1\n",
      "layer1_2_conv3\n",
      "layer1_2_bn3\n",
      "add_2\n",
      "layer1_2_relu_2\n",
      "layer2_0_conv1\n",
      "layer2_0_bn1\n",
      "layer2_0_relu\n",
      "layer2_0_conv2\n",
      "layer2_0_bn2\n",
      "layer2_0_relu_1\n",
      "layer2_0_conv3\n",
      "layer2_0_bn3\n",
      "layer2_0_downsample_0\n",
      "layer2_0_downsample_1\n",
      "add_3\n",
      "layer2_0_relu_2\n",
      "layer2_1_conv1\n",
      "layer2_1_bn1\n",
      "layer2_1_relu\n",
      "layer2_1_conv2\n",
      "layer2_1_bn2\n",
      "layer2_1_relu_1\n",
      "layer2_1_conv3\n",
      "layer2_1_bn3\n",
      "add_4\n",
      "layer2_1_relu_2\n",
      "layer2_2_conv1\n",
      "layer2_2_bn1\n",
      "layer2_2_relu\n",
      "layer2_2_conv2\n",
      "layer2_2_bn2\n",
      "layer2_2_relu_1\n",
      "layer2_2_conv3\n",
      "layer2_2_bn3\n",
      "add_5\n",
      "layer2_2_relu_2\n",
      "layer2_3_conv1\n",
      "layer2_3_bn1\n",
      "layer2_3_relu\n",
      "layer2_3_conv2\n",
      "layer2_3_bn2\n",
      "layer2_3_relu_1\n",
      "layer2_3_conv3\n",
      "layer2_3_bn3\n",
      "add_6\n",
      "layer2_3_relu_2\n",
      "layer3_0_conv1\n",
      "layer3_0_bn1\n",
      "layer3_0_relu\n",
      "layer3_0_conv2\n",
      "layer3_0_bn2\n",
      "layer3_0_relu_1\n",
      "layer3_0_conv3\n",
      "layer3_0_bn3\n",
      "layer3_0_downsample_0\n",
      "layer3_0_downsample_1\n",
      "add_7\n",
      "layer3_0_relu_2\n",
      "layer3_1_conv1\n",
      "layer3_1_bn1\n",
      "layer3_1_relu\n",
      "layer3_1_conv2\n",
      "layer3_1_bn2\n",
      "layer3_1_relu_1\n",
      "layer3_1_conv3\n",
      "layer3_1_bn3\n",
      "add_8\n",
      "layer3_1_relu_2\n",
      "layer3_2_conv1\n",
      "layer3_2_bn1\n",
      "layer3_2_relu\n",
      "layer3_2_conv2\n",
      "layer3_2_bn2\n",
      "layer3_2_relu_1\n",
      "layer3_2_conv3\n",
      "layer3_2_bn3\n",
      "add_9\n",
      "layer3_2_relu_2\n",
      "layer3_3_conv1\n",
      "layer3_3_bn1\n",
      "layer3_3_relu\n",
      "layer3_3_conv2\n",
      "layer3_3_bn2\n",
      "layer3_3_relu_1\n",
      "layer3_3_conv3\n",
      "layer3_3_bn3\n",
      "add_10\n",
      "layer3_3_relu_2\n",
      "layer3_4_conv1\n",
      "layer3_4_bn1\n",
      "layer3_4_relu\n",
      "layer3_4_conv2\n",
      "layer3_4_bn2\n",
      "layer3_4_relu_1\n",
      "layer3_4_conv3\n",
      "layer3_4_bn3\n",
      "add_11\n",
      "layer3_4_relu_2\n",
      "layer3_5_conv1\n",
      "layer3_5_bn1\n",
      "layer3_5_relu\n",
      "layer3_5_conv2\n",
      "layer3_5_bn2\n",
      "layer3_5_relu_1\n",
      "layer3_5_conv3\n",
      "layer3_5_bn3\n",
      "add_12\n",
      "layer3_5_relu_2\n",
      "layer4_0_conv1\n",
      "layer4_0_bn1\n",
      "layer4_0_relu\n",
      "layer4_0_conv2\n",
      "layer4_0_bn2\n",
      "layer4_0_relu_1\n",
      "layer4_0_conv3\n",
      "layer4_0_bn3\n",
      "layer4_0_downsample_0\n",
      "layer4_0_downsample_1\n",
      "add_13\n",
      "layer4_0_relu_2\n",
      "layer4_1_conv1\n",
      "layer4_1_bn1\n",
      "layer4_1_relu\n",
      "layer4_1_conv2\n",
      "layer4_1_bn2\n",
      "layer4_1_relu_1\n",
      "layer4_1_conv3\n",
      "layer4_1_bn3\n",
      "add_14\n",
      "layer4_1_relu_2\n",
      "layer4_2_conv1\n",
      "layer4_2_bn1\n",
      "layer4_2_relu\n",
      "layer4_2_conv2\n",
      "layer4_2_bn2\n",
      "layer4_2_relu_1\n",
      "layer4_2_conv3\n",
      "layer4_2_bn3\n",
      "add_15\n",
      "layer4_2_relu_2\n",
      "avgpool\n",
      "flatten\n",
      "fc\n",
      "output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ATL(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Module(\n",
       "      (0): Module(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Module(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Module(\n",
       "      (0): Module(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Module(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Module(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Module(\n",
       "      (0): Module(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Module(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): Module(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Module(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): Module(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Module(\n",
       "      (0): Module(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Module(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Module(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (fcl): Linear(in_features=1984, out_features=5, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fx as fx\n",
    "import numpy as np\n",
    "from torch.fx import symbolic_trace\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "class ATL(nn.Module):\n",
    "    def __init__(self, train, Nclasses, Nlayer=3, Pmax=0.4):\n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        layers = self.get_layers(train)\n",
    "        \n",
    "        fm_indicies, resnet_out_size = self.get_featuremaps_idicies(train,layers)        \n",
    "        self.resnet = self._transform(self.resnet, Nlayer, layers, fm_indicies)\n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fcl = nn.Linear(resnet_out_size, Nclasses)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "        \n",
    "    def get_layers(self, train):\n",
    "        # Musi zwracać coś w tym stylu, tj iterowalny obiekt z N conv layers\n",
    "        # Musi zwracać coś w tym stylu, tj iterowalny obiekt z N conv layers\n",
    "        model_weights =[]\n",
    "        layers = []\n",
    "        model_children = list(self.resnet.children())\n",
    "        counter = 0\n",
    "        for i in range(len(model_children)):\n",
    "            if type(model_children[i]) == nn.Conv2d:\n",
    "                counter+=1\n",
    "                model_weights.append(model_children[i].weight)\n",
    "                layers.append(model_children[i])\n",
    "            elif type(model_children[i]) == nn.Sequential:\n",
    "                for j in range(len(model_children[i])):\n",
    "                    for child in model_children[i][j].children():\n",
    "                        if type(child) == nn.Conv2d:\n",
    "                            counter+=1\n",
    "                            model_weights.append(child.weight)\n",
    "                            layers.append(child)\n",
    "        outputs = []\n",
    "        names = []\n",
    "        for layer in layers[0:]:\n",
    "            train = layer(train)\n",
    "            outputs.append(train)\n",
    "            names.append(str(layer))\n",
    "        # max pooling\n",
    "        max_pool = []\n",
    "        for feature_map in outputs:\n",
    "            m = torch.nn.MaxPool1d(2, stride=2)\n",
    "            max_pool.append(m(feature_map))\n",
    "        # LAV\n",
    "        for layers in \n",
    "        # centroids of classes C_k\n",
    "        # layers’ relevance scores (R)\n",
    "        # select top-N relevant layers\n",
    "        return [3,9,20] # najlepiej indeksy warstw konwolucyjnych \n",
    "    \n",
    "    def get_featuremaps_idicies(self, train, layers, N_feature  = 3):\n",
    "        \n",
    "        # obecnie bez pilnowania finalnej liczyby wybranych feature map\n",
    "        # bez pilnownaia zeby z każdej klasy było n_feature map \n",
    "        # bez obliczania progu p value dla każdej klasy w warstwie (ustalona sztywna wartość 0.05)\n",
    "        \n",
    "        images, labels = trainset \n",
    "        labels_set = set(np.asarray(labels))\n",
    "                \n",
    "        conv_layers,model_weights = self.get_resnet_conv_layers()\n",
    "    \n",
    "        choose_fm = []\n",
    "        choose_fm_output_len=0\n",
    "        # iteruje po warstwach konwolucyjnuch \n",
    "        for conv_ind in layers:\n",
    "            choose_fm_curent_conv_layer = []\n",
    "            \n",
    "            # iteruje po mapach             \n",
    "            for fm_ind in range(len(model_weights[conv_ind])):                \n",
    "                LAV_vec = []\n",
    "                \n",
    "                \n",
    "                for image in images:\n",
    "                    out_fm = self.get_feature_map_outputs(image)\n",
    "                    \n",
    "                    LAV_vec.append(self.LAV(out_fm[conv_ind][fm_ind]))\n",
    "                 \n",
    "                for _class in labels_set:\n",
    "                    LAV_vec_curent_class = []\n",
    "                    LAV_vec_other_class = []\n",
    "                    for i,fm in enumerate(LAV_vec):\n",
    "                        if labels[i]==_class:\n",
    "                            LAV_vec_curent_class.append(fm)\n",
    "                        else:     \n",
    "                            LAV_vec_other_class.append(fm)\n",
    "                    \n",
    "                    # zwraca nan przy LAV_vec_current_class długości 1 !!\n",
    "                    t_stat, p = ttest_ind(LAV_vec_curent_class, LAV_vec_other_class)\n",
    "                    #print(\"t_stat, p\", t_stat, p )\n",
    "                    \n",
    "                    # TODO pl = p_max * Rl/Rmax\n",
    "                    p_treshold = 0.05 # testowo\n",
    "                    \n",
    "                    if p < p_treshold:\n",
    "                        choose_fm_curent_conv_layer.append(fm_ind)\n",
    "                        \n",
    "                        map_dim = out_fm[conv_ind][fm_ind]\n",
    "                        #print(map_dim)\n",
    "                        \n",
    "                        choose_fm_output_len+= map_dim[0].size()[0] **2\n",
    "                        \n",
    "                        print((conv_ind,fm_ind))\n",
    "            choose_fm.append(choose_fm_curent_conv_layer)\n",
    "        \n",
    "        print (choose_fm)\n",
    "        \n",
    "        # Musi zwracać obiekt który ma N iterowalnych rzeczy, z których każda ma ileś indeksów feature map. \n",
    "        # W sumie ilość indeksów musi być równa ilości klas * Nfeature z 2.3 w artykule\n",
    "        #return [[1],[1]], 1568\n",
    "        \n",
    "        return choose_fm, choose_fm_output_len  # na razie zwraca coś takiego: [[30, 94], [8, 100, 155, 187, 188], [49, 51, 90]]\n",
    "        \n",
    "\n",
    "    \n",
    "    def LAV(self,featureMap):\n",
    "        return featureMap.max().detach().numpy().item(0)\n",
    "    \n",
    "    def _transform(self, m: torch.nn.Module, n, layers, idx) -> torch.nn.Module:\n",
    "        gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n",
    "        graph=gm.graph\n",
    "        blocks_in_layers = [1,3,4,6,3]\n",
    "        idx_to_layer_name = ['conv1'] + [f\"layer{i}_{j}_conv{k}\" for i in range(1,5) for j in range(blocks_in_layers[i]) for k in range(1,4)]\n",
    "        layer_names = [idx_to_layer_name[idx] for idx in layers]\n",
    "        final_nodes=[]\n",
    "        last_node=None\n",
    "\n",
    "        for node in graph.nodes:\n",
    "            if node.name in layer_names:\n",
    "                final_nodes.append(node)\n",
    "            if not last_node and len(final_nodes)==n:\n",
    "                last_node = node\n",
    "            if node.name == 'output':\n",
    "                out_node = node\n",
    "\n",
    "        i=0\n",
    "        nodes_to_output=[]\n",
    "        for i in range(n):\n",
    "            with graph.inserting_after(last_node):\n",
    "            # Insert a new `call_function` node calling `torch.relu`\n",
    "                new_node = graph.call_function(torch.tensor,\n",
    "                                               args=(idx[i],),\n",
    "                                              kwargs={\"dtype\":torch.int32})\n",
    "                last_node = new_node\n",
    "\n",
    "            with graph.inserting_after(last_node):\n",
    "                new_node = graph.call_function(torch.index_select,\n",
    "                                              args=(final_nodes[i], 1, last_node))\n",
    "                last_node = new_node\n",
    "\n",
    "            with graph.inserting_after(last_node):\n",
    "                new_node = graph.call_function(torch.flatten,\n",
    "                                              args=(last_node,1))\n",
    "                nodes_to_output.append(new_node)\n",
    "                last_node = new_node\n",
    "        with graph.inserting_after(last_node):\n",
    "                new_node = graph.call_function(torch.cat,\n",
    "                                              args=(nodes_to_output,1))\n",
    "        out_node.args=(new_node,)\n",
    "        graph.eliminate_dead_code()\n",
    "        graph.lint() \n",
    "        gm.recompile()\n",
    "\n",
    "        return gm\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fcl(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_resnet_conv_layers(self):\n",
    "        model = self.resnet\n",
    "        # we will save the conv layer weights in this list\n",
    "        model_weights =[]\n",
    "\n",
    "        #we will save the 49 conv layers in this list\n",
    "        conv_layers = []\n",
    "\n",
    "        # get all the model children as list\n",
    "        model_children = list(model.children())\n",
    "\n",
    "        #counter to keep count of the conv layers\n",
    "        counter = 0\n",
    "        #append all the conv layers and their respective wights to the list\n",
    "\n",
    "        for i in range(len(model_children)):\n",
    "\n",
    "            if type(model_children[i]) == nn.Conv2d:\n",
    "                counter+=1\n",
    "                model_weights.append(model_children[i].weight)\n",
    "                conv_layers.append(model_children[i])\n",
    "\n",
    "            elif type(model_children[i]) == nn.Sequential:\n",
    "                for j in range(len(model_children[i])):\n",
    "                    for child in model_children[i][j].children():\n",
    "                        if type(child) == nn.Conv2d:\n",
    "                            counter+=1\n",
    "                            model_weights.append(child.weight)\n",
    "                            conv_layers.append(child)\n",
    "\n",
    "        #print(f\"Total convolution layers: {counter}\")\n",
    "        #print(\"conv_layers\")\n",
    "        return (conv_layers,model_weights)\n",
    "    \n",
    "    def get_feature_map_outputs(self,image):\n",
    "        conv_layers, _ = self.get_resnet_conv_layers()\n",
    "        outputs = []\n",
    "        names = []\n",
    "\n",
    "        for layer in conv_layers[0:]:\n",
    "            image = layer(image)\n",
    "            outputs.append(image)\n",
    "            names.append(str(layer))\n",
    "            \n",
    "        #print(len(outputs))\n",
    "\n",
    "        # print feature_maps\n",
    "        #for feature_map in outputs:\n",
    "            #print(feature_map.shape)\n",
    "            \n",
    "        return outputs;   \n",
    "\n",
    "\n",
    "# testy\n",
    "transform = transforms.Compose([            #[1]\n",
    "             transforms.Resize(256),                    #[2]\n",
    "             transforms.CenterCrop(224),                #[3]\n",
    "             transforms.ToTensor(),                     #[4]\n",
    "             transforms.Normalize(                      #[5]\n",
    "             mean=[0.485, 0.456, 0.406],                #[6]\n",
    "             std=[0.229, 0.224, 0.225]                  #[7]\n",
    "             )])\n",
    "\n",
    "\n",
    "#img = Image.open(\"dog.jpg\")\n",
    "#img = Image.open('img/cat.jpg')\n",
    "#img_t = transform(img)\n",
    "#batch_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR10 \n",
    "import torchvision\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "trainset = images, labels\n",
    "#print(trainset)\n",
    "\n",
    "\n",
    "m = ATL(trainset, 5, 2)\n",
    "m.eval()\n",
    "\n",
    "# out=m(batch_t)\n",
    "# out.size()\n",
    "\n",
    "#print(symbolic_trace(m.resnet).code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
